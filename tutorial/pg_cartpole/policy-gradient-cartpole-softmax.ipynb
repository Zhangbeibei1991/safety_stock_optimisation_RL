{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "LEARNING_RATE = 0.000025\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "nA = env.action_space.n\n",
    "\n",
    "nA = 30**4\n",
    "nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(4, nA)\n",
    "w.shape, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep stats for rewards\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, w):\n",
    "    z = state.dot(w)\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def softmax_grad(softmax):\n",
    "    s = softmax.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()[None, :]\n",
    "state.shape, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = policy(state, w)\n",
    "probs.shape, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.choice(nA, p=probs[0])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsoftmax = softmax_grad(probs)[action, :]\n",
    "dsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlog = dsoftmax / probs[0, action]\n",
    "dlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = state.T.dot(dlog[None, :])\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w += LEARNING_RATE * grad * sum([r * (GAMMA**r) for t, r in enumerate(rewards)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-790eee5d8c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "action =  np.array([random.uniform(-2,2)])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eek31/Documents/openai/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "gamma = 0.2 # 0.8\n",
    "alpha  =0.1 \n",
    "beta  = 0.2\n",
    "\n",
    "theta = np.random.rand(2,1)\n",
    "w = np.random.rand(3,1)\n",
    "sigma  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.40807509,  0.        ]), array([[0.88246626],\n",
       "        [0.35778724]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, theta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ref = https://stackoverflow.com/questions/46872856/implementing-policy-gradient-algorithm-in-actor-critic-framework-from-david-silv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:42<00:00, 47.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([nan]), array([nan])] [array([nan]), array([nan]), array([nan])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## policy improvement  - Critic\n",
    "def Q_approx(state_action,w):\n",
    "    Q = np.dot(np.transpose(state_action),w)\n",
    "    return Q\n",
    "\n",
    "## policy evaluation - Actor\n",
    "def policy_approx(state,theta):\n",
    "    action = np.dot(np.transpose(state),theta)\n",
    "    return(action)\n",
    "\n",
    "num_episodes = 2000\n",
    "num_steps  = 300\n",
    "rt =  np.zeros([num_steps,num_episodes]) \n",
    "a_save = np.zeros([num_steps,num_episodes])\n",
    "state_history = np.zeros([num_steps,num_episodes])\n",
    "\n",
    "for i in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    e = (0.8/num_episodes) * i + 0.1\n",
    "\n",
    "    if(random.random() > e):\n",
    "        action =  np.array([random.uniform(-2,2)])\n",
    "    else:\n",
    "        action = policy_approx(state,theta)\n",
    "\n",
    "    for j in range(num_steps):\n",
    "        ## generate action\n",
    "\n",
    "        ## take action\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        rt[j,i] =  reward\n",
    "        a_save[j,i] = action\n",
    "\n",
    "        a = np.random.normal(action,sigma)\n",
    "        if(random.random() > e):\n",
    "             new_action =  random.uniform(-2,2)\n",
    "        else:\n",
    "            new_action = policy_approx(state,theta)\n",
    "            \n",
    "        new_action = policy_approx(new_state,theta)\n",
    "        state_action = [state[0],state[1],action[0]]        \n",
    "        new_state_action = [new_state[0],new_state[1],new_action[0]]\n",
    "        \n",
    "        ## calculate temporal difference\n",
    "        state_history[j,i] = state[0]\n",
    "        delta = reward + gamma*Q_approx(new_state_action,w) - Q_approx(state_action,w)\n",
    "\n",
    "        theta =[x + y for x,y in zip(theta,alpha *  state *((a - action)/sigma**2)*Q_approx(state_action,w))]\n",
    "\n",
    "        w = [x + y for x, y in zip(w, beta * delta * state_action)]\n",
    "\n",
    "\n",
    "        state =  new_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"done\")\n",
    "            break\n",
    "        \n",
    "#         assert abs(action[0]) <= 2\n",
    "        \n",
    "print(theta, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([nan]), array([nan])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = policy_approx(state,theta)\n",
    "    new_state,reward,done,info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
